{
  "function": {
    "description": "Perform speaker diarisation on an audio or video file using pyannote-audio, returning timestamped speaker turns. Optionally merge with a Whisper transcript to produce speaker-attributed segments.",
    "name": "media.diarize_audio",
    "parameters": {
      "additionalProperties": false,
      "properties": {
        "audio_path": {
          "description": "Path to an audio or video file (any format readable by ffmpeg).",
          "type": "string"
        },
        "context_hint": {
          "default": null,
          "description": "Optional context to help the LLM identify speakers (e.g., event name, known attendees, company). Only used when identify_speakers is true.",
          "type": [
            "string",
            "null"
          ]
        },
        "device": {
          "default": null,
          "description": "PyTorch device for local models (e.g., 'cuda', 'cpu', 'mps'). Auto-detected if null. Ignored for cloud models.",
          "type": [
            "string",
            "null"
          ]
        },
        "identify_speakers": {
          "default": false,
          "description": "When true and transcript_json is provided, use an LLM (OpenAI) to identify speaker names from contextual clues in the transcript (introductions, hand-overs, slide references). Requires OPENAI_API_KEY.",
          "type": "boolean"
        },
        "max_speakers": {
          "default": null,
          "description": "Maximum expected number of speakers.",
          "minimum": 1,
          "type": [
            "integer",
            "null"
          ]
        },
        "min_speakers": {
          "default": null,
          "description": "Minimum expected number of speakers.",
          "minimum": 1,
          "type": [
            "integer",
            "null"
          ]
        },
        "model": {
          "default": "pyannote/speaker-diarization-community-1",
          "description": "Pyannote pipeline name. Use 'pyannote/speaker-diarization-community-1' (free, local) or 'pyannote/speaker-diarization-precision-2' (cloud API).",
          "type": "string"
        },
        "num_speakers": {
          "default": null,
          "description": "Exact number of speakers if known.",
          "minimum": 1,
          "type": [
            "integer",
            "null"
          ]
        },
        "output_dir": {
          "default": null,
          "description": "Directory for output files. Defaults to ./<audio_stem>_diarization.",
          "type": [
            "string",
            "null"
          ]
        },
        "token": {
          "default": null,
          "description": "HuggingFace token (for community models) or pyannoteAI API key (for precision models). Falls back to HF_TOKEN or PYANNOTE_API_KEY environment variables.",
          "type": [
            "string",
            "null"
          ]
        },
        "transcript_json": {
          "default": null,
          "description": "Optional path to a transcript.verbose.json from media.analyze_video. When provided, output includes speaker-attributed transcript segments and SRT.",
          "type": [
            "string",
            "null"
          ]
        }
      },
      "required": [
        "audio_path"
      ],
      "type": "object"
    }
  },
  "type": "function"
}
